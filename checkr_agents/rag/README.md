# Milvus Context Generator for AllyCat files

This context generator piggy-backs off of the AllyCat project to serve the vector database context generated 
by that project in the context of CheckrAgents.  This makes it easy to use the web scraping, chunking and 
vector store insertion from that project in the context of this project.

- https://github.com/The-AI-Alliance/allycat

The AllyCat project uses [llama\_index](https://github.com/run-llama/llama_index) to generate the vector database.  Here, in the interest of
paring the code down to its essentials, we incorporate the Milvus database directly.  To do so, 
it is necessary to understand the database fields and node (File Object) used by llama\_index.

This is a long way of saying that this is not a general Milvus database context generator, but one 
specifically designed for Milvus databases generated by llama\_index for the AllyCat project.


## To Generate a DB file of your own

This project comes with a file named "rag\_website\_milvus.db" in the resources directory.  This one
was generated by scraping https://mclarenlabs.com, a website that knows about MIDI (Musical Instrument
Digital Interface).  You can use AllyCat to scrape a different website of your choosing and use the 
resulting Milvus DB file.

Here are the basic steps for doing that.

- Clone the allycat project and set up the project

        $ cd allycat
        $ uv venv --python 3.11
        $ . .venv/bin/activate
        $ uv pip install -r requirements.txt

- Configure the website to crawl and parameters in `my_config.py`.

- Now run the first three steps.

        $ python 1_crawl_site.py
        $ python 2_process_files.py
        $ python 3_save_to_vector_db.py
	
- copy the DB file to this project and make sure the Vector Database parameters and embedding model name from the AllyCat project are reflected in the file `milvus_context_generator.py` in this directory.


## A note about the embedding server

This project works well with ollama serving the embedding model locally.  Make sure you have ollama
installed and also install the appropriate embedding model.

     $ ollama pull granite-embedding:30m
